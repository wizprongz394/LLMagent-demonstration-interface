# Agent Demonstration Interface

An explainable evaluation framework for **LLM-powered GIS agents**.  
This system evaluates large language models not just by final answers, but by **intent understanding, reasoning behavior, execution efficiency, and policy-driven performance trade-offs**.

The framework supports multi-model comparison (Gemma-3, Mistral, LLaMA-3), reasoning trace replay, dynamic scoring, and an interactive Streamlit interface.

---

## ğŸ” Key Features

- LLM-based **intent classification**
- ReAct-style **agent reasoning pipeline**
- **Multi-model benchmarking** (Gemma-3, Mistral, LLaMA-3)
- Policy-driven **dynamic scoring**
- **Step-by-step reasoning trace replay**
- Query-level and model-level comparison
- Interactive **3D Streamlit UI**
- Explainable, research-ready design

---

## ğŸ“ Project Structure

```
AgentDemonstrationInterface/
â”‚
â”œâ”€â”€ run.py                     # Streamlit interface (main entry point)
â”‚
â”œâ”€â”€ enhanced_react_agent.py    # Intent-aware GIS agent with traceable reasoning
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ models.yaml            # Model definitions (Gemma, Mistral, LLaMA)
â”‚   â””â”€â”€ evaluation.yaml        # Scoring thresholds and defaults
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dummy6_queries.json    # Benchmark GIS queries
â”‚   â””â”€â”€ results/               # Generated experiment results (gitignored)
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ scorer.py              # Policy-based scoring logic
â”‚   â”œâ”€â”€ metrics.py             # Performance metrics
â”‚   â””â”€â”€ reasoning_metrics.py   # Reasoning quality metrics
â”‚
â”œâ”€â”€ runners/
â”‚   â”œâ”€â”€ model_runner.py        # Runs one model on one query
â”‚   â””â”€â”€ batch_runner.py        # Runs all models on all queries
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ intent_classifier.py   # LLM-based intent classification
â”‚   â”œâ”€â”€ timers.py              # Timing utilities
â”‚   â””â”€â”€ validators.py          # Safety and schema checks
â”‚
â””â”€â”€ .gitignore
```

---

## ğŸ§  System Workflow

1. User queries are loaded from a benchmark dataset.
2. Each query is classified by the LLM to infer **intent**.
3. An enhanced ReAct agent performs reasoning and generates observations.
4. Execution traces and metrics are logged.
5. Models are scored using a **policy-weighted evaluation function**.
6. Results are visualized in an interactive Streamlit interface.

---

## âš™ï¸ Prerequisites

- **Python 3.10+**
- **Git**
- **Ollama** (for running local LLMs)

---

## ğŸ§© Install Ollama & Models

### 1ï¸âƒ£ Install Ollama

Download from: https://ollama.com

Ensure it is running:

```bash
ollama serve
```

### 2ï¸âƒ£ Pull required models

```bash
ollama pull gemma3:latest
ollama pull mistral:latest
ollama pull llama3:latest
```

Verify:

```bash
ollama list
```

---

## ğŸ Python Setup

### 1ï¸âƒ£ Create virtual environment (recommended)

```bash
python -m venv venv
```

Activate:

```bash
# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

If `requirements.txt` is not present:

```bash
pip install streamlit pandas langchain langchain-ollama pyyaml
```

---

## ğŸš€ Run Experiments (Generate Results)

From the project root:

```bash
python -m runners.batch_runner
```

This will:

- Run all models on all benchmark queries
- Generate a timestamped JSON file in: `data/results/`

---

## ğŸ–¥ï¸ Launch the Interface

```bash
python -m streamlit run run.py
```

Open your browser at: `http://localhost:8501`

---

## ğŸ›ï¸ Using the Interface

- Select an experiment run from the sidebar
- Adjust scoring policy weights dynamically
- Compare models across intent, latency, and efficiency
- Drill down to query-level behavior
- Replay agent reasoning step by step
- Observe how model preference changes with policy

---

## ğŸ§ª Notes on Observations

The current system generates **model-based geospatial observations** (textual reasoning outputs). The architecture is designed to plug in real GIS APIs or spatial databases without changing the evaluation pipeline.

---

## ğŸ“ Academic Use & Evaluation

This project is suitable for:

- Research demonstrations
- Model comparison studies
- Explainable AI experiments
- Agentic LLM system evaluation

---

## ğŸ“Œ License

This project is intended for **academic and research use**.
